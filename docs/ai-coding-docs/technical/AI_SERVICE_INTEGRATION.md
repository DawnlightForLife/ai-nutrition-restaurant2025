# AIæ™ºèƒ½è¥å…»é¤å…ç³»ç»Ÿ - AIæœåŠ¡é›†æˆé…ç½®

> **æ–‡æ¡£ç‰ˆæœ¬**: 1.0.0  
> **åˆ›å»ºæ—¥æœŸ**: 2025-07-13  
> **æ›´æ–°æ—¥æœŸ**: 2025-07-13  
> **æ–‡æ¡£çŠ¶æ€**: âœ… å¼€å‘å°±ç»ª  
> **ç›®æ ‡å—ä¼—**: åç«¯å¼€å‘å›¢é˜Ÿã€AIé›†æˆå·¥ç¨‹å¸ˆã€DevOpså›¢é˜Ÿ

## ğŸ“‹ ç›®å½•

- [1. AIæœåŠ¡æ¶æ„æ¦‚è¿°](#1-aiæœåŠ¡æ¶æ„æ¦‚è¿°)
- [2. LangChainé›†æˆé…ç½®](#2-langchainé›†æˆé…ç½®)
- [3. DeepSeek APIé›†æˆ](#3-deepseek-apié›†æˆ)
- [4. å‘é‡æ•°æ®åº“é…ç½®](#4-å‘é‡æ•°æ®åº“é…ç½®)
- [5. AIæ¨èæœåŠ¡](#5-aiæ¨èæœåŠ¡)
- [6. å¯¹è¯ç³»ç»Ÿé…ç½®](#6-å¯¹è¯ç³»ç»Ÿé…ç½®)
- [7. ç›‘æ§å’Œæ—¥å¿—](#7-ç›‘æ§å’Œæ—¥å¿—)
- [8. å®‰å…¨é…ç½®](#8-å®‰å…¨é…ç½®)

---

## 1. AIæœåŠ¡æ¶æ„æ¦‚è¿°

### 1.1 AIæœåŠ¡æŠ€æœ¯æ ˆ

```yaml
AIæŠ€æœ¯æ¶æ„:
  æ ¸å¿ƒæ¡†æ¶:
    LangChain: 0.0.145+
      - åŠŸèƒ½: å¤§æ¨¡å‹åº”ç”¨æ¡†æ¶
      - ç”¨é€”: å¯¹è¯é“¾ç®¡ç†ã€Promptå·¥ç¨‹
      - é›†æˆ: DeepSeek APIã€å‘é‡æ•°æ®åº“
      
    DeepSeek API: v1
      - åŠŸèƒ½: å¤§è¯­è¨€æ¨¡å‹æ¨ç†
      - æ¨¡å‹: deepseek-chatã€deepseek-coder
      - ç”¨é€”: è¥å…»å’¨è¯¢ã€æ™ºèƒ½æ¨è
      
  å‘é‡å­˜å‚¨:
    pgvector: 0.5.0+
      - åŠŸèƒ½: å‘é‡æ•°æ®å­˜å‚¨å’Œæ£€ç´¢
      - ç»´åº¦: 1536 (OpenAIå…¼å®¹)
      - ç´¢å¼•: IVFFlat, HNSW
      
  è¾…åŠ©å·¥å…·:
    OpenAI SDK: 4.24.0+
      - åŠŸèƒ½: å‘é‡åŒ–ã€å…¼å®¹æ¥å£
      - æ¨¡å‹: text-embedding-ada-002
      - ç”¨é€”: æ–‡æœ¬å‘é‡åŒ–
      
æœåŠ¡éƒ¨ç½²æ¶æ„:
  AI Gateway:
    - ç»Ÿä¸€AIæœåŠ¡å…¥å£
    - è¯·æ±‚è·¯ç”±å’Œè´Ÿè½½å‡è¡¡
    - APIå¯†é’¥ç®¡ç†
    - é™æµå’Œç¼“å­˜
    
  æ¨èå¼•æ“:
    - ä¸ªæ€§åŒ–æ¨èç®—æ³•
    - å‘é‡ç›¸ä¼¼åº¦è®¡ç®—
    - å®æ—¶æ¨èæœåŠ¡
    - æ¨èç»“æœç¼“å­˜
    
  å¯¹è¯æœåŠ¡:
    - è¥å…»å¸ˆAIåŠ©æ‰‹
    - ç”¨æˆ·å’¨è¯¢æœºå™¨äºº
    - ä¼šè¯çŠ¶æ€ç®¡ç†
    - ä¸Šä¸‹æ–‡è®°å¿†
```

### 1.2 AIæœåŠ¡äº¤äº’æµç¨‹

```mermaid
graph TB
    subgraph "å‰ç«¯åº”ç”¨"
        A[ç”¨æˆ·ç•Œé¢]
        B[å•†å®¶åå°]
    end
    
    subgraph "AI Gateway"
        C[è¯·æ±‚è·¯ç”±]
        D[è®¤è¯æˆæƒ]
        E[é™æµæ§åˆ¶]
    end
    
    subgraph "AIæœåŠ¡å±‚"
        F[æ¨èå¼•æ“]
        G[å¯¹è¯æœåŠ¡]
        H[å‘é‡æœç´¢]
    end
    
    subgraph "å¤–éƒ¨AIæœåŠ¡"
        I[DeepSeek API]
        J[OpenAI API]
    end
    
    subgraph "æ•°æ®å­˜å‚¨"
        K[(PostgreSQL + pgvector)]
        L[(Redis Cache)]
    end
    
    A --> C
    B --> C
    C --> D
    D --> E
    E --> F
    E --> G
    F --> H
    G --> I
    H --> J
    F --> K
    G --> K
    H --> K
    F --> L
    G --> L
```

---

## 2. LangChainé›†æˆé…ç½®

### 2.1 LangChainç¯å¢ƒé…ç½®

```typescript
// config/langchain.config.ts
export interface LangChainConfig {
  // DeepSeeké…ç½®
  deepseek: {
    apiKey: string;
    baseURL: string;
    model: string;
    maxTokens: number;
    temperature: number;
    timeout: number;
  };
  
  // OpenAIé…ç½® (ç”¨äºå‘é‡åŒ–)
  openai: {
    apiKey: string;
    embeddingModel: string;
    maxRetries: number;
    timeout: number;
  };
  
  // å‘é‡æ•°æ®åº“é…ç½®
  vectorStore: {
    tableName: string;
    dimensions: number;
    indexType: 'ivfflat' | 'hnsw';
    indexParams: Record<string, any>;
  };
  
  // ç¼“å­˜é…ç½®
  cache: {
    enabled: boolean;
    ttl: number;
    maxSize: number;
    keyPrefix: string;
  };
}

export const langChainConfig: LangChainConfig = {
  deepseek: {
    apiKey: process.env.DEEPSEEK_API_KEY,
    baseURL: process.env.DEEPSEEK_API_BASE_URL || 'https://api.deepseek.com',
    model: process.env.DEEPSEEK_MODEL || 'deepseek-chat',
    maxTokens: parseInt(process.env.DEEPSEEK_MAX_TOKENS) || 2048,
    temperature: parseFloat(process.env.DEEPSEEK_TEMPERATURE) || 0.7,
    timeout: parseInt(process.env.DEEPSEEK_TIMEOUT) || 30000,
  },
  
  openai: {
    apiKey: process.env.OPENAI_API_KEY,
    embeddingModel: process.env.OPENAI_EMBEDDING_MODEL || 'text-embedding-ada-002',
    maxRetries: parseInt(process.env.OPENAI_MAX_RETRIES) || 3,
    timeout: parseInt(process.env.OPENAI_TIMEOUT) || 30000,
  },
  
  vectorStore: {
    tableName: process.env.VECTOR_TABLE_NAME || 'vector_embeddings',
    dimensions: parseInt(process.env.VECTOR_DIMENSIONS) || 1536,
    indexType: (process.env.VECTOR_INDEX_TYPE as any) || 'ivfflat',
    indexParams: {
      lists: parseInt(process.env.VECTOR_INDEX_LISTS) || 100,
      m: parseInt(process.env.VECTOR_INDEX_M) || 16,
      efConstruction: parseInt(process.env.VECTOR_INDEX_EF_CONSTRUCTION) || 64,
    },
  },
  
  cache: {
    enabled: process.env.LANGCHAIN_CACHE_ENABLED !== 'false',
    ttl: parseInt(process.env.LANGCHAIN_CACHE_TTL) || 3600,
    maxSize: parseInt(process.env.LANGCHAIN_CACHE_MAX_SIZE) || 1000,
    keyPrefix: process.env.LANGCHAIN_CACHE_PREFIX || 'lc:',
  },
};
```

### 2.2 LangChainæœåŠ¡å®ç°

```typescript
// services/langchain.service.ts
import { Injectable, Logger } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { ChatOpenAI } from '@langchain/openai';
import { OpenAIEmbeddings } from '@langchain/openai';
import { PGVectorStore } from '@langchain/community/vectorstores/pgvector';
import { ConversationChain } from 'langchain/chains';
import { BufferWindowMemory } from 'langchain/memory';
import { PromptTemplate } from '@langchain/core/prompts';
import { Pool } from 'pg';

@Injectable()
export class LangChainService {
  private readonly logger = new Logger(LangChainService.name);
  private readonly config: LangChainConfig;
  private readonly chatModel: ChatOpenAI;
  private readonly embeddings: OpenAIEmbeddings;
  private readonly vectorStore: PGVectorStore;
  private readonly dbPool: Pool;

  constructor(private configService: ConfigService) {
    this.config = this.configService.get<LangChainConfig>('langchain');
    this.initializeServices();
  }

  private async initializeServices() {
    try {
      // åˆå§‹åŒ–DeepSeek Chatæ¨¡å‹
      this.chatModel = new ChatOpenAI({
        openAIApiKey: this.config.deepseek.apiKey,
        configuration: {
          baseURL: this.config.deepseek.baseURL,
        },
        modelName: this.config.deepseek.model,
        maxTokens: this.config.deepseek.maxTokens,
        temperature: this.config.deepseek.temperature,
        timeout: this.config.deepseek.timeout,
      });

      // åˆå§‹åŒ–OpenAI Embeddings
      this.embeddings = new OpenAIEmbeddings({
        openAIApiKey: this.config.openai.apiKey,
        modelName: this.config.openai.embeddingModel,
        maxRetries: this.config.openai.maxRetries,
        timeout: this.config.openai.timeout,
      });

      // åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± 
      this.dbPool = new Pool({
        connectionString: process.env.DATABASE_URL,
        max: 10,
        idleTimeoutMillis: 30000,
        connectionTimeoutMillis: 2000,
      });

      // åˆå§‹åŒ–å‘é‡å­˜å‚¨
      this.vectorStore = await PGVectorStore.initialize(this.embeddings, {
        pool: this.dbPool,
        tableName: this.config.vectorStore.tableName,
        columns: {
          idColumnName: 'id',
          vectorColumnName: 'embedding',
          contentColumnName: 'content',
          metadataColumnName: 'metadata',
        },
        distanceStrategy: 'cosine',
      });

      this.logger.log('LangChainæœåŠ¡åˆå§‹åŒ–å®Œæˆ');
    } catch (error) {
      this.logger.error('LangChainæœåŠ¡åˆå§‹åŒ–å¤±è´¥', error);
      throw error;
    }
  }

  /**
   * è¥å…»å’¨è¯¢å¯¹è¯
   */
  async nutritionConsultation(
    sessionId: string,
    userMessage: string,
    userProfile?: any,
  ): Promise<NutritionChatResponse> {
    try {
      // æ„å»ºè¥å…»å’¨è¯¢Prompt
      const nutritionPrompt = PromptTemplate.fromTemplate(`
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¥å…»å¸ˆAIåŠ©æ‰‹ï¼Œä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–çš„è¥å…»å»ºè®®ã€‚

ç”¨æˆ·æ¡£æ¡ˆï¼š
{userProfile}

å¯¹è¯å†å²ï¼š
{chatHistory}

ç”¨æˆ·é—®é¢˜ï¼š{userMessage}

è¯·åŸºäºç”¨æˆ·çš„ä¸ªäººä¿¡æ¯å’Œé¥®é£Ÿç›®æ ‡ï¼Œæä¾›ä¸“ä¸šã€å®ç”¨çš„è¥å…»å»ºè®®ã€‚å›ç­”è¦åŒ…æ‹¬ï¼š
1. é’ˆå¯¹æ€§çš„è¥å…»åˆ†æ
2. å…·ä½“çš„é¥®é£Ÿå»ºè®®
3. æ¨èçš„èœå“ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
4. åç»­é—®é¢˜å»ºè®®

å›ç­”è¦ä¸“ä¸šä½†æ˜“æ‡‚ï¼Œé¿å…è¿‡äºæŠ€æœ¯æ€§çš„æœ¯è¯­ã€‚
`);

      // åˆ›å»ºå¯¹è¯é“¾
      const memory = new BufferWindowMemory({
        k: 10, // ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
        memoryKey: 'chatHistory',
        returnMessages: true,
      });

      // åŠ è½½å†å²å¯¹è¯
      await this.loadChatHistory(sessionId, memory);

      const chain = new ConversationChain({
        llm: this.chatModel,
        memory,
        prompt: nutritionPrompt,
      });

      // æ‰§è¡Œå¯¹è¯
      const response = await chain.call({
        userMessage,
        userProfile: JSON.stringify(userProfile || {}),
      });

      // ä¿å­˜å¯¹è¯è®°å½•
      await this.saveChatMessage(sessionId, userMessage, response.response);

      // æå–æ¨èèœå“
      const dishRecommendations = await this.extractDishRecommendations(
        response.response,
        userProfile,
      );

      this.logger.log(`è¥å…»å’¨è¯¢å®Œæˆ - ä¼šè¯ID: ${sessionId}`);

      return {
        sessionId,
        response: response.response,
        dishRecommendations,
        followUpQuestions: this.generateFollowUpQuestions(response.response),
        timestamp: new Date(),
      };
    } catch (error) {
      this.logger.error(`è¥å…»å’¨è¯¢å¤±è´¥ - ä¼šè¯ID: ${sessionId}`, error);
      throw error;
    }
  }

  /**
   * èœå“æ¨è
   */
  async dishRecommendation(
    userProfile: any,
    preferences: any,
    location?: { lat: number; lng: number },
  ): Promise<DishRecommendationResponse> {
    try {
      // æ„å»ºç”¨æˆ·åå¥½å‘é‡
      const preferenceText = this.buildPreferenceText(userProfile, preferences);
      const preferenceVector = await this.embeddings.embedQuery(preferenceText);

      // å‘é‡ç›¸ä¼¼åº¦æœç´¢
      const similarDishes = await this.vectorStore.similaritySearch(
        preferenceText,
        10,
        {
          entityType: 'dish',
          isAvailable: true,
        },
      );

      // ä½¿ç”¨AIé‡æ–°æ’åºå’Œä¸ªæ€§åŒ–
      const personalizedRecommendations = await this.personalizeRecommendations(
        similarDishes,
        userProfile,
        preferences,
      );

      // æ·»åŠ åœ°ç†ä½ç½®ç­›é€‰
      const locationFilteredDishes = await this.filterByLocation(
        personalizedRecommendations,
        location,
      );

      this.logger.log(`èœå“æ¨èå®Œæˆ - ç”¨æˆ·: ${userProfile.userId}`);

      return {
        recommendations: locationFilteredDishes,
        explanation: await this.generateRecommendationExplanation(
          locationFilteredDishes,
          userProfile,
        ),
        confidence: this.calculateConfidence(locationFilteredDishes),
        timestamp: new Date(),
      };
    } catch (error) {
      this.logger.error('èœå“æ¨èå¤±è´¥', error);
      throw error;
    }
  }

  /**
   * å­˜å‚¨æ–‡æ¡£å‘é‡
   */
  async storeDocumentEmbedding(
    content: string,
    metadata: any,
  ): Promise<string> {
    try {
      const documents = [
        {
          pageContent: content,
          metadata,
        },
      ];

      const ids = await this.vectorStore.addDocuments(documents);
      
      this.logger.log(`æ–‡æ¡£å‘é‡å­˜å‚¨æˆåŠŸ - ID: ${ids[0]}`);
      return ids[0];
    } catch (error) {
      this.logger.error('æ–‡æ¡£å‘é‡å­˜å‚¨å¤±è´¥', error);
      throw error;
    }
  }

  /**
   * æ‰¹é‡æ›´æ–°èœå“å‘é‡
   */
  async updateDishEmbeddings(dishes: any[]): Promise<void> {
    try {
      const batchSize = 50;
      
      for (let i = 0; i < dishes.length; i += batchSize) {
        const batch = dishes.slice(i, i + batchSize);
        
        const documents = batch.map(dish => ({
          pageContent: this.buildDishText(dish),
          metadata: {
            entityType: 'dish',
            dishId: dish.id,
            storeId: dish.storeId,
            category: dish.category,
            nutrition: dish.nutrition,
            tags: dish.tags,
            isAvailable: dish.isAvailable,
            updatedAt: new Date().toISOString(),
          },
        }));

        await this.vectorStore.addDocuments(documents);
        
        this.logger.log(`æ‰¹é‡æ›´æ–°èœå“å‘é‡ - æ‰¹æ¬¡ ${Math.floor(i / batchSize) + 1}`);
      }
      
      this.logger.log(`èœå“å‘é‡æ‰¹é‡æ›´æ–°å®Œæˆ - æ€»è®¡: ${dishes.length}`);
    } catch (error) {
      this.logger.error('èœå“å‘é‡æ‰¹é‡æ›´æ–°å¤±è´¥', error);
      throw error;
    }
  }

  // ç§æœ‰è¾…åŠ©æ–¹æ³•
  private async loadChatHistory(sessionId: string, memory: BufferWindowMemory): Promise<void> {
    // ä»æ•°æ®åº“åŠ è½½å†å²å¯¹è¯
    const history = await this.getChatHistory(sessionId);
    
    for (const message of history) {
      if (message.role === 'user') {
        await memory.chatMemory.addUserMessage(message.content);
      } else {
        await memory.chatMemory.addAIChatMessage(message.content);
      }
    }
  }

  private async saveChatMessage(
    sessionId: string,
    userMessage: string,
    aiResponse: string,
  ): Promise<void> {
    // ä¿å­˜å¯¹è¯è®°å½•åˆ°æ•°æ®åº“
    // å®ç°å…·ä½“çš„æ•°æ®åº“æ“ä½œ
  }

  private buildPreferenceText(userProfile: any, preferences: any): string {
    const parts = [];
    
    if (userProfile?.dietaryRestrictions?.length) {
      parts.push(`é¥®é£Ÿé™åˆ¶: ${userProfile.dietaryRestrictions.join(', ')}`);
    }
    
    if (preferences?.cuisineTypes?.length) {
      parts.push(`èœç³»åå¥½: ${preferences.cuisineTypes.join(', ')}`);
    }
    
    if (preferences?.maxCalories) {
      parts.push(`æœ€å¤§çƒ­é‡: ${preferences.maxCalories}å¡è·¯é‡Œ`);
    }
    
    if (userProfile?.healthGoals?.length) {
      parts.push(`å¥åº·ç›®æ ‡: ${userProfile.healthGoals.join(', ')}`);
    }
    
    return parts.join('; ');
  }

  private buildDishText(dish: any): string {
    const parts = [
      `èœå“åç§°: ${dish.name}`,
      `æè¿°: ${dish.description}`,
      `åˆ†ç±»: ${dish.category}`,
    ];
    
    if (dish.nutrition) {
      parts.push(`è¥å…»ä¿¡æ¯: çƒ­é‡${dish.nutrition.calories}å¡è·¯é‡Œ, è›‹ç™½è´¨${dish.nutrition.protein}g, ç¢³æ°´${dish.nutrition.carbs}g, è„‚è‚ª${dish.nutrition.fat}g`);
    }
    
    if (dish.tags?.length) {
      parts.push(`æ ‡ç­¾: ${dish.tags.join(', ')}`);
    }
    
    if (dish.ingredients?.length) {
      const mainIngredients = dish.ingredients
        .filter(ing => ing.isMain)
        .map(ing => ing.name)
        .join(', ');
      parts.push(`ä¸»è¦é£Ÿæ: ${mainIngredients}`);
    }
    
    return parts.join('; ');
  }

  private async personalizeRecommendations(
    dishes: any[],
    userProfile: any,
    preferences: any,
  ): Promise<any[]> {
    // ä½¿ç”¨AIå¯¹æ¨èç»“æœè¿›è¡Œä¸ªæ€§åŒ–æ’åº
    const personalizePrompt = `
åŸºäºç”¨æˆ·æ¡£æ¡ˆå’Œåå¥½ï¼Œå¯¹ä»¥ä¸‹èœå“è¿›è¡Œä¸ªæ€§åŒ–æ’åºå’Œè¯„åˆ†ï¼š

ç”¨æˆ·æ¡£æ¡ˆï¼š${JSON.stringify(userProfile)}
ç”¨æˆ·åå¥½ï¼š${JSON.stringify(preferences)}

èœå“åˆ—è¡¨ï¼š
${dishes.map((dish, index) => `${index + 1}. ${dish.pageContent}`).join('\n')}

è¯·è¿”å›JSONæ ¼å¼çš„æ’åºç»“æœï¼ŒåŒ…å«æ¨èç†ç”±å’ŒåŒ¹é…åº¦è¯„åˆ†ï¼ˆ0-1ï¼‰ã€‚
`;

    try {
      const response = await this.chatModel.call([
        { role: 'user', content: personalizePrompt },
      ]);
      
      // è§£æAIå“åº”å¹¶é‡æ–°æ’åº
      const personalizedResults = JSON.parse(response.content);
      return personalizedResults;
    } catch (error) {
      this.logger.warn('ä¸ªæ€§åŒ–æ’åºå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ’åº', error);
      return dishes.map((dish, index) => ({
        ...dish,
        score: 1 - index * 0.1,
        reason: 'åŸºäºå‘é‡ç›¸ä¼¼åº¦æ¨è',
      }));
    }
  }

  private async filterByLocation(
    dishes: any[],
    location?: { lat: number; lng: number },
  ): Promise<any[]> {
    if (!location) return dishes;
    
    // æ ¹æ®ä½ç½®ç­›é€‰é™„è¿‘çš„é¤å…
    // è¿™é‡Œåº”è¯¥ç»“åˆå®é™…çš„åœ°ç†ä½ç½®æŸ¥è¯¢é€»è¾‘
    return dishes;
  }

  private async generateRecommendationExplanation(
    dishes: any[],
    userProfile: any,
  ): Promise<string> {
    const explanationPrompt = `
ä¸ºç”¨æˆ·è§£é‡Šä¸ºä»€ä¹ˆæ¨èè¿™äº›èœå“ï¼š

ç”¨æˆ·æ¡£æ¡ˆï¼š${JSON.stringify(userProfile)}
æ¨èèœå“ï¼š${dishes.slice(0, 3).map(dish => dish.name).join(', ')}

è¯·ç”¨ç®€æ´å‹å¥½çš„è¯­è¨€è§£é‡Šæ¨èç†ç”±ã€‚
`;

    try {
      const response = await this.chatModel.call([
        { role: 'user', content: explanationPrompt },
      ]);
      
      return response.content;
    } catch (error) {
      this.logger.warn('ç”Ÿæˆæ¨èè§£é‡Šå¤±è´¥', error);
      return 'åŸºäºæ‚¨çš„åå¥½å’Œè¥å…»éœ€æ±‚ï¼Œæˆ‘ä»¬ä¸ºæ‚¨ç²¾é€‰äº†è¿™äº›èœå“ã€‚';
    }
  }

  private calculateConfidence(recommendations: any[]): number {
    if (!recommendations.length) return 0;
    
    const avgScore = recommendations.reduce((sum, rec) => sum + (rec.score || 0), 0) / recommendations.length;
    return Math.round(avgScore * 100) / 100;
  }

  private generateFollowUpQuestions(response: string): string[] {
    // åŸºäºAIå“åº”ç”Ÿæˆåç»­é—®é¢˜å»ºè®®
    const commonQuestions = [
      'æ‚¨è¿˜æœ‰å…¶ä»–é¥®é£Ÿé™åˆ¶å—ï¼Ÿ',
      'æ‚¨å¸Œæœ›äº†è§£å…·ä½“çš„çƒ¹é¥ªæ–¹æ³•å—ï¼Ÿ',
      'æ‚¨éœ€è¦åˆ¶å®šä¸€å‘¨çš„é¥®é£Ÿè®¡åˆ’å—ï¼Ÿ',
      'æ‚¨æƒ³äº†è§£å¦‚ä½•æ­é…å…¶ä»–é£Ÿç‰©å—ï¼Ÿ',
    ];
    
    return commonQuestions.slice(0, 2);
  }

  private async getChatHistory(sessionId: string): Promise<any[]> {
    // ä»æ•°æ®åº“è·å–èŠå¤©å†å²
    const query = `
      SELECT role, content, created_at
      FROM ai_messages 
      WHERE session_id = $1 
      ORDER BY created_at ASC 
      LIMIT 20
    `;
    
    try {
      const result = await this.dbPool.query(query, [sessionId]);
      return result.rows;
    } catch (error) {
      this.logger.error('è·å–èŠå¤©å†å²å¤±è´¥', error);
      return [];
    }
  }
}

// ç±»å‹å®šä¹‰
interface NutritionChatResponse {
  sessionId: string;
  response: string;
  dishRecommendations?: any[];
  followUpQuestions: string[];
  timestamp: Date;
}

interface DishRecommendationResponse {
  recommendations: any[];
  explanation: string;
  confidence: number;
  timestamp: Date;
}
```

---

## 3. DeepSeek APIé›†æˆ

### 3.1 DeepSeek APIé…ç½®

```typescript
// config/deepseek.config.ts
export interface DeepSeekConfig {
  apiKey: string;
  baseURL: string;
  models: {
    chat: string;
    coder: string;
    embedding: string;
  };
  limits: {
    maxTokens: number;
    maxRequestsPerMinute: number;
    maxRequestsPerDay: number;
    timeout: number;
  };
  retry: {
    maxRetries: number;
    backoffMultiplier: number;
    maxBackoffTime: number;
  };
  cache: {
    enabled: boolean;
    ttl: number;
    keyPrefix: string;
  };
}

export const deepSeekConfig: DeepSeekConfig = {
  apiKey: process.env.DEEPSEEK_API_KEY,
  baseURL: process.env.DEEPSEEK_API_BASE_URL || 'https://api.deepseek.com',
  
  models: {
    chat: process.env.DEEPSEEK_CHAT_MODEL || 'deepseek-chat',
    coder: process.env.DEEPSEEK_CODER_MODEL || 'deepseek-coder',
    embedding: process.env.DEEPSEEK_EMBEDDING_MODEL || 'deepseek-embedding',
  },
  
  limits: {
    maxTokens: parseInt(process.env.DEEPSEEK_MAX_TOKENS) || 4096,
    maxRequestsPerMinute: parseInt(process.env.DEEPSEEK_RPM_LIMIT) || 60,
    maxRequestsPerDay: parseInt(process.env.DEEPSEEK_RPD_LIMIT) || 10000,
    timeout: parseInt(process.env.DEEPSEEK_TIMEOUT) || 30000,
  },
  
  retry: {
    maxRetries: parseInt(process.env.DEEPSEEK_MAX_RETRIES) || 3,
    backoffMultiplier: parseFloat(process.env.DEEPSEEK_BACKOFF_MULTIPLIER) || 2,
    maxBackoffTime: parseInt(process.env.DEEPSEEK_MAX_BACKOFF) || 60000,
  },
  
  cache: {
    enabled: process.env.DEEPSEEK_CACHE_ENABLED !== 'false',
    ttl: parseInt(process.env.DEEPSEEK_CACHE_TTL) || 3600,
    keyPrefix: process.env.DEEPSEEK_CACHE_PREFIX || 'ds:',
  },
};
```

### 3.2 DeepSeekæœåŠ¡å®ç°

```typescript
// services/deepseek.service.ts
import { Injectable, Logger } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { HttpService } from '@nestjs/axios';
import { RedisService } from './redis.service';
import { DeepSeekConfig } from '../config/deepseek.config';
import { AxiosRequestConfig, AxiosError } from 'axios';

@Injectable()
export class DeepSeekService {
  private readonly logger = new Logger(DeepSeekService.name);
  private readonly config: DeepSeekConfig;
  private requestCount = 0;
  private lastResetTime = Date.now();

  constructor(
    private configService: ConfigService,
    private httpService: HttpService,
    private redisService: RedisService,
  ) {
    this.config = this.configService.get<DeepSeekConfig>('deepseek');
  }

  /**
   * è¥å…»åˆ†æå¯¹è¯
   */
  async nutritionAnalysis(
    userInput: string,
    nutritionData: any,
    userProfile?: any,
  ): Promise<DeepSeekResponse> {
    const prompt = this.buildNutritionAnalysisPrompt(userInput, nutritionData, userProfile);
    
    return this.chatCompletion({
      model: this.config.models.chat,
      messages: [
        {
          role: 'system',
          content: 'ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¥å…»å¸ˆï¼Œæ“…é•¿åˆ†æé£Ÿç‰©è¥å…»æˆåˆ†å¹¶æä¾›å¥åº·å»ºè®®ã€‚è¯·ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€å›ç­”ç”¨æˆ·é—®é¢˜ã€‚',
        },
        {
          role: 'user',
          content: prompt,
        },
      ],
      temperature: 0.7,
      max_tokens: 1024,
    });
  }

  /**
   * èœå“æè¿°ç”Ÿæˆ
   */
  async generateDishDescription(
    dishName: string,
    ingredients: string[],
    nutritionInfo: any,
    tags?: string[],
  ): Promise<DeepSeekResponse> {
    const prompt = `
è¯·ä¸ºä»¥ä¸‹èœå“ç”Ÿæˆä¸€æ®µå¸å¼•äººçš„æè¿°ï¼š

èœå“åç§°ï¼š${dishName}
ä¸»è¦é£Ÿæï¼š${ingredients.join(', ')}
è¥å…»ä¿¡æ¯ï¼šçƒ­é‡${nutritionInfo.calories}å¡è·¯é‡Œï¼Œè›‹ç™½è´¨${nutritionInfo.protein}gï¼Œç¢³æ°´${nutritionInfo.carbs}gï¼Œè„‚è‚ª${nutritionInfo.fat}g
ç‰¹è‰²æ ‡ç­¾ï¼š${tags ? tags.join(', ') : 'æ— '}

è¯·ç”Ÿæˆä¸€æ®µ50-100å­—çš„èœå“æè¿°ï¼Œçªå‡ºè¥å…»ä»·å€¼å’Œå£æ„Ÿç‰¹è‰²ï¼Œè¯­è¨€è¦ç”ŸåŠ¨æœ‰å¸å¼•åŠ›ã€‚
`;

    return this.chatCompletion({
      model: this.config.models.chat,
      messages: [
        {
          role: 'system',
          content: 'ä½ æ˜¯ä¸€ä½èµ„æ·±ç¾é£Ÿæ–‡æ¡ˆä¸“å®¶ï¼Œæ“…é•¿æ’°å†™å¸å¼•äººçš„èœå“æè¿°ã€‚',
        },
        {
          role: 'user',
          content: prompt,
        },
      ],
      temperature: 0.8,
      max_tokens: 200,
    });
  }

  /**
   * æ™ºèƒ½åº“å­˜ç®¡ç†å»ºè®®
   */
  async inventoryManagementAdvice(
    currentStock: any[],
    salesHistory: any[],
    seasonalFactors?: any,
  ): Promise<DeepSeekResponse> {
    const prompt = this.buildInventoryAnalysisPrompt(currentStock, salesHistory, seasonalFactors);
    
    return this.chatCompletion({
      model: this.config.models.chat,
      messages: [
        {
          role: 'system',
          content: 'ä½ æ˜¯ä¸€ä½åº“å­˜ç®¡ç†ä¸“å®¶ï¼Œæ“…é•¿åˆ†æé”€å”®æ•°æ®å’Œåº“å­˜æƒ…å†µï¼Œæä¾›æ™ºèƒ½è¡¥è´§å»ºè®®ã€‚',
        },
        {
          role: 'user',
          content: prompt,
        },
      ],
      temperature: 0.3,
      max_tokens: 1024,
    });
  }

  /**
   * è¥å…»è®¡åˆ’åˆ¶å®š
   */
  async createNutritionPlan(
    userProfile: any,
    goals: string[],
    preferences: any,
    restrictions: string[],
  ): Promise<DeepSeekResponse> {
    const prompt = `
è¯·ä¸ºç”¨æˆ·åˆ¶å®šä¸€ä¸ªä¸ªæ€§åŒ–çš„è¥å…»è®¡åˆ’ï¼š

ç”¨æˆ·ä¿¡æ¯ï¼š
- å¹´é¾„ï¼š${userProfile.age}å²
- æ€§åˆ«ï¼š${userProfile.gender}
- èº«é«˜ï¼š${userProfile.height}cm
- ä½“é‡ï¼š${userProfile.weight}kg
- æ´»åŠ¨æ°´å¹³ï¼š${userProfile.activityLevel}
- ç›®æ ‡ä½“é‡ï¼š${userProfile.targetWeight || 'ç»´æŒå½“å‰ä½“é‡'}kg

å¥åº·ç›®æ ‡ï¼š${goals.join(', ')}
é¥®é£Ÿåå¥½ï¼š${JSON.stringify(preferences)}
é¥®é£Ÿé™åˆ¶ï¼š${restrictions.join(', ')}

è¯·åˆ¶å®šä¸€ä¸ª7å¤©çš„è¥å…»è®¡åˆ’ï¼ŒåŒ…æ‹¬ï¼š
1. æ¯æ—¥è¥å…»ç›®æ ‡ï¼ˆå¡è·¯é‡Œã€è›‹ç™½è´¨ã€ç¢³æ°´ã€è„‚è‚ªï¼‰
2. ä¸€æ—¥ä¸‰é¤çš„å»ºè®®æ­é…
3. å¥åº·å°è´´å£«
4. æ³¨æ„äº‹é¡¹

è¯·ç”¨JSONæ ¼å¼è¿”å›ï¼Œç»“æ„æ¸…æ™°æ˜“äºè§£æã€‚
`;

    return this.chatCompletion({
      model: this.config.models.chat,
      messages: [
        {
          role: 'system',
          content: 'ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¥å…»å¸ˆï¼Œç²¾é€šåˆ¶å®šä¸ªæ€§åŒ–è¥å…»è®¡åˆ’ã€‚è¯·æä¾›ç§‘å­¦ã€å®ç”¨çš„å»ºè®®ã€‚',
        },
        {
          role: 'user',
          content: prompt,
        },
      ],
      temperature: 0.5,
      max_tokens: 2048,
    });
  }

  /**
   * é€šç”¨èŠå¤©å®Œæˆæ¥å£
   */
  async chatCompletion(request: DeepSeekChatRequest): Promise<DeepSeekResponse> {
    // æ£€æŸ¥ç¼“å­˜
    if (this.config.cache.enabled) {
      const cacheKey = this.generateCacheKey(request);
      const cachedResponse = await this.redisService.get(cacheKey);
      
      if (cachedResponse) {
        this.logger.log('è¿”å›ç¼“å­˜çš„DeepSeekå“åº”');
        return JSON.parse(cachedResponse);
      }
    }

    // é™æµæ£€æŸ¥
    await this.checkRateLimit();

    try {
      const response = await this.makeRequest('/chat/completions', request);
      
      // ç¼“å­˜å“åº”
      if (this.config.cache.enabled && response.choices?.[0]?.message) {
        const cacheKey = this.generateCacheKey(request);
        await this.redisService.setex(
          cacheKey,
          this.config.cache.ttl,
          JSON.stringify(response),
        );
      }

      this.logger.log(`DeepSeek APIè°ƒç”¨æˆåŠŸ - æ¨¡å‹: ${request.model}`);
      return response;
      
    } catch (error) {
      this.logger.error('DeepSeek APIè°ƒç”¨å¤±è´¥', error);
      throw this.handleAPIError(error);
    }
  }

  /**
   * å¥åº·æ£€æŸ¥
   */
  async healthCheck(): Promise<boolean> {
    try {
      const response = await this.chatCompletion({
        model: this.config.models.chat,
        messages: [
          {
            role: 'user',
            content: 'Hello, this is a health check.',
          },
        ],
        max_tokens: 10,
      });

      return !!response.choices?.[0]?.message;
    } catch (error) {
      this.logger.error('DeepSeekå¥åº·æ£€æŸ¥å¤±è´¥', error);
      return false;
    }
  }

  // ç§æœ‰æ–¹æ³•
  private async makeRequest(endpoint: string, data: any): Promise<any> {
    const config: AxiosRequestConfig = {
      method: 'POST',
      url: `${this.config.baseURL}${endpoint}`,
      headers: {
        'Authorization': `Bearer ${this.config.apiKey}`,
        'Content-Type': 'application/json',
      },
      data,
      timeout: this.config.limits.timeout,
    };

    for (let attempt = 0; attempt <= this.config.retry.maxRetries; attempt++) {
      try {
        const response = await this.httpService.request(config).toPromise();
        return response.data;
      } catch (error) {
        if (attempt === this.config.retry.maxRetries) {
          throw error;
        }

        const backoffTime = Math.min(
          1000 * Math.pow(this.config.retry.backoffMultiplier, attempt),
          this.config.retry.maxBackoffTime,
        );

        this.logger.warn(`DeepSeek APIé‡è¯• - å°è¯• ${attempt + 1}/${this.config.retry.maxRetries + 1}ï¼Œ${backoffTime}msåé‡è¯•`);
        await this.sleep(backoffTime);
      }
    }
  }

  private async checkRateLimit(): Promise<void> {
    const now = Date.now();
    
    // é‡ç½®è®¡æ•°å™¨ï¼ˆæ¯åˆ†é’Ÿï¼‰
    if (now - this.lastResetTime > 60000) {
      this.requestCount = 0;
      this.lastResetTime = now;
    }

    // æ£€æŸ¥åˆ†é’Ÿé™åˆ¶
    if (this.requestCount >= this.config.limits.maxRequestsPerMinute) {
      const waitTime = 60000 - (now - this.lastResetTime);
      throw new Error(`DeepSeek APIåˆ†é’Ÿé™æµï¼Œè¯·ç­‰å¾… ${Math.ceil(waitTime / 1000)} ç§’`);
    }

    this.requestCount++;
  }

  private generateCacheKey(request: DeepSeekChatRequest): string {
    const key = JSON.stringify({
      model: request.model,
      messages: request.messages,
      temperature: request.temperature,
      max_tokens: request.max_tokens,
    });
    
    return `${this.config.cache.keyPrefix}${this.hashString(key)}`;
  }

  private hashString(str: string): string {
    // ç®€å•çš„å“ˆå¸Œå‡½æ•°
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // è½¬æ¢ä¸º32ä½æ•´æ•°
    }
    return Math.abs(hash).toString(36);
  }

  private buildNutritionAnalysisPrompt(
    userInput: string,
    nutritionData: any,
    userProfile?: any,
  ): string {
    return `
ç”¨æˆ·é—®é¢˜ï¼š${userInput}

è¥å…»æ•°æ®ï¼š
${JSON.stringify(nutritionData, null, 2)}

${userProfile ? `ç”¨æˆ·æ¡£æ¡ˆï¼š
- å¹´é¾„ï¼š${userProfile.age}å²
- æ€§åˆ«ï¼š${userProfile.gender}
- BMIï¼š${userProfile.bmi || 'æœªçŸ¥'}
- æ´»åŠ¨æ°´å¹³ï¼š${userProfile.activityLevel || 'æœªçŸ¥'}
- å¥åº·ç›®æ ‡ï¼š${userProfile.goals?.join(', ') || 'æœªçŸ¥'}
- é¥®é£Ÿé™åˆ¶ï¼š${userProfile.restrictions?.join(', ') || 'æ— '}` : ''}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æä¾›ä¸“ä¸šçš„è¥å…»åˆ†æå’Œå»ºè®®ã€‚
`;
  }

  private buildInventoryAnalysisPrompt(
    currentStock: any[],
    salesHistory: any[],
    seasonalFactors?: any,
  ): string {
    return `
å½“å‰åº“å­˜æƒ…å†µï¼š
${currentStock.map(item => `${item.name}: åº“å­˜${item.quantity}${item.unit}, å®‰å…¨åº“å­˜${item.safetyStock}${item.unit}`).join('\n')}

é”€å”®å†å²ï¼ˆæœ€è¿‘30å¤©ï¼‰ï¼š
${salesHistory.map(record => `${record.date}: ${record.itemName} é”€å”®${record.quantity}${record.unit}`).join('\n')}

${seasonalFactors ? `å­£èŠ‚æ€§å› ç´ ï¼š
${JSON.stringify(seasonalFactors, null, 2)}` : ''}

è¯·åˆ†æåº“å­˜æƒ…å†µå¹¶æä¾›ä»¥ä¸‹å»ºè®®ï¼š
1. å“ªäº›å•†å“éœ€è¦ç«‹å³è¡¥è´§
2. å»ºè®®çš„è¡¥è´§æ•°é‡
3. æœªæ¥ä¸€å‘¨çš„é”€å”®é¢„æµ‹
4. åº“å­˜ä¼˜åŒ–å»ºè®®

è¯·ç”¨JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚
`;
  }

  private handleAPIError(error: any): Error {
    if (error.response) {
      const status = error.response.status;
      const message = error.response.data?.error?.message || error.message;
      
      switch (status) {
        case 401:
          return new Error('DeepSeek APIè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥');
        case 429:
          return new Error('DeepSeek APIè¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åé‡è¯•');
        case 500:
          return new Error('DeepSeek APIæœåŠ¡å™¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•');
        default:
          return new Error(`DeepSeek APIé”™è¯¯ (${status}): ${message}`);
      }
    }
    
    return new Error(`DeepSeek APIè¯·æ±‚å¤±è´¥: ${error.message}`);
  }

  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

// ç±»å‹å®šä¹‰
interface DeepSeekChatRequest {
  model: string;
  messages: Array<{
    role: 'system' | 'user' | 'assistant';
    content: string;
  }>;
  temperature?: number;
  max_tokens?: number;
  top_p?: number;
  frequency_penalty?: number;
  presence_penalty?: number;
  stop?: string[];
}

interface DeepSeekResponse {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Array<{
    index: number;
    message: {
      role: string;
      content: string;
    };
    finish_reason: string;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}
```

---

## 4. å‘é‡æ•°æ®åº“é…ç½®

### 4.1 pgvectoræ‰©å±•é…ç½®

```sql
-- PostgreSQL + pgvector é…ç½®

-- å¯ç”¨pgvectoræ‰©å±•
CREATE EXTENSION IF NOT EXISTS vector;
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- åˆ›å»ºå‘é‡åµŒå…¥è¡¨
CREATE TABLE IF NOT EXISTS vector_embeddings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    entity_type VARCHAR(50) NOT NULL,
    entity_id VARCHAR(255) NOT NULL,
    content TEXT NOT NULL,
    embedding vector(1536) NOT NULL,
    metadata JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    CONSTRAINT unique_entity_embedding UNIQUE(entity_type, entity_id)
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_vector_embeddings_entity_type 
ON vector_embeddings(entity_type);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_vector_embeddings_entity_id 
ON vector_embeddings(entity_id);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_vector_embeddings_created_at 
ON vector_embeddings(created_at DESC);

-- åˆ›å»ºå‘é‡ç›¸ä¼¼åº¦ç´¢å¼• (IVFFlat)
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_vector_embeddings_embedding_ivfflat
ON vector_embeddings USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- åˆ›å»ºå‘é‡ç›¸ä¼¼åº¦ç´¢å¼• (HNSW) - æ›´å¥½çš„æŸ¥è¯¢æ€§èƒ½
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_vector_embeddings_embedding_hnsw
ON vector_embeddings USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- åˆ›å»ºå¤åˆç´¢å¼• (å®ä½“ç±»å‹ + å‘é‡)
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_vector_embeddings_type_embedding
ON vector_embeddings(entity_type) INCLUDE (embedding);

-- æ›´æ–°æ—¶é—´è§¦å‘å™¨
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_vector_embeddings_updated_at 
    BEFORE UPDATE ON vector_embeddings 
    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();

-- å‘é‡æœç´¢ä¼˜åŒ–å‡½æ•°
CREATE OR REPLACE FUNCTION find_similar_dishes(
    query_embedding vector(1536),
    similarity_threshold FLOAT DEFAULT 0.8,
    result_limit INT DEFAULT 10,
    store_ids UUID[] DEFAULT NULL
) RETURNS TABLE (
    dish_id VARCHAR(255),
    similarity FLOAT,
    content TEXT,
    metadata JSONB
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        ve.entity_id as dish_id,
        1 - (ve.embedding <=> query_embedding) as similarity,
        ve.content,
        ve.metadata
    FROM vector_embeddings ve
    WHERE ve.entity_type = 'dish'
        AND (store_ids IS NULL OR (ve.metadata->>'storeId')::UUID = ANY(store_ids))
        AND (ve.metadata->>'isAvailable')::boolean = true
        AND (1 - (ve.embedding <=> query_embedding)) >= similarity_threshold
    ORDER BY ve.embedding <=> query_embedding
    LIMIT result_limit;
END;
$$ LANGUAGE plpgsql;

-- å‘é‡åµŒå…¥ç»Ÿè®¡è§†å›¾
CREATE OR REPLACE VIEW vector_embeddings_stats AS
SELECT 
    entity_type,
    COUNT(*) as total_embeddings,
    COUNT(*) FILTER (WHERE created_at > NOW() - INTERVAL '24 hours') as new_today,
    COUNT(*) FILTER (WHERE updated_at > NOW() - INTERVAL '24 hours') as updated_today,
    MIN(created_at) as first_created,
    MAX(updated_at) as last_updated
FROM vector_embeddings
GROUP BY entity_type;

-- æ€§èƒ½ç›‘æ§æŸ¥è¯¢
CREATE OR REPLACE VIEW vector_performance_stats AS
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan as index_scans,
    idx_tup_read as tuples_read,
    idx_tup_fetch as tuples_fetched
FROM pg_stat_user_indexes 
WHERE tablename = 'vector_embeddings'
ORDER BY idx_scan DESC;
```

### 4.2 å‘é‡æ“ä½œæœåŠ¡

```typescript
// services/vector.service.ts
import { Injectable, Logger } from '@nestjs/common';
import { InjectRepository } from '@nestjs/typeorm';
import { Repository } from 'typeorm';
import { VectorEmbedding } from '../entities/vector-embedding.entity';
import { OpenAIEmbeddings } from '@langchain/openai';

@Injectable()
export class VectorService {
  private readonly logger = new Logger(VectorService.name);
  private readonly embeddings: OpenAIEmbeddings;

  constructor(
    @InjectRepository(VectorEmbedding)
    private vectorRepository: Repository<VectorEmbedding>,
  ) {
    this.embeddings = new OpenAIEmbeddings({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: 'text-embedding-ada-002',
    });
  }

  /**
   * ç”Ÿæˆå¹¶å­˜å‚¨å®ä½“å‘é‡
   */
  async createEmbedding(
    entityType: string,
    entityId: string,
    content: string,
    metadata?: any,
  ): Promise<VectorEmbedding> {
    try {
      // ç”Ÿæˆå‘é‡
      const embeddingVector = await this.embeddings.embedQuery(content);
      
      // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
      const existing = await this.vectorRepository.findOne({
        where: { entityType, entityId },
      });

      let vectorEmbedding: VectorEmbedding;
      
      if (existing) {
        // æ›´æ–°ç°æœ‰å‘é‡
        existing.content = content;
        existing.embedding = embeddingVector;
        existing.metadata = metadata;
        vectorEmbedding = await this.vectorRepository.save(existing);
        this.logger.log(`æ›´æ–°å‘é‡åµŒå…¥: ${entityType}/${entityId}`);
      } else {
        // åˆ›å»ºæ–°å‘é‡
        vectorEmbedding = this.vectorRepository.create({
          entityType,
          entityId,
          content,
          embedding: embeddingVector,
          metadata,
        });
        vectorEmbedding = await this.vectorRepository.save(vectorEmbedding);
        this.logger.log(`åˆ›å»ºå‘é‡åµŒå…¥: ${entityType}/${entityId}`);
      }

      return vectorEmbedding;
    } catch (error) {
      this.logger.error(`åˆ›å»ºå‘é‡åµŒå…¥å¤±è´¥: ${entityType}/${entityId}`, error);
      throw error;
    }
  }

  /**
   * ç›¸ä¼¼åº¦æœç´¢
   */
  async similaritySearch(
    queryText: string,
    entityType: string,
    limit: number = 10,
    threshold: number = 0.8,
    filters?: any,
  ): Promise<SimilaritySearchResult[]> {
    try {
      // ç”ŸæˆæŸ¥è¯¢å‘é‡
      const queryEmbedding = await this.embeddings.embedQuery(queryText);
      
      // æ„å»ºSQLæŸ¥è¯¢
      let query = `
        SELECT 
          entity_id,
          content,
          metadata,
          1 - (embedding <=> $1::vector) as similarity
        FROM vector_embeddings
        WHERE entity_type = $2
          AND (1 - (embedding <=> $1::vector)) >= $3
      `;
      
      const params: any[] = [JSON.stringify(queryEmbedding), entityType, threshold];
      let paramIndex = 4;

      // æ·»åŠ è¿‡æ»¤æ¡ä»¶
      if (filters) {
        Object.entries(filters).forEach(([key, value]) => {
          query += ` AND metadata->>'${key}' = $${paramIndex}`;
          params.push(value);
          paramIndex++;
        });
      }

      query += ` ORDER BY embedding <=> $1::vector LIMIT $${paramIndex}`;
      params.push(limit);

      const results = await this.vectorRepository.query(query, params);
      
      this.logger.log(`ç›¸ä¼¼åº¦æœç´¢å®Œæˆ: æŸ¥è¯¢"${queryText}", è¿”å›${results.length}æ¡ç»“æœ`);
      
      return results.map(row => ({
        entityId: row.entity_id,
        content: row.content,
        metadata: row.metadata,
        similarity: parseFloat(row.similarity),
      }));
    } catch (error) {
      this.logger.error('ç›¸ä¼¼åº¦æœç´¢å¤±è´¥', error);
      throw error;
    }
  }

  /**
   * æ‰¹é‡åˆ›å»ºå‘é‡
   */
  async batchCreateEmbeddings(
    items: Array<{
      entityType: string;
      entityId: string;
      content: string;
      metadata?: any;
    }>,
  ): Promise<void> {
    const batchSize = 100;
    const totalBatches = Math.ceil(items.length / batchSize);
    
    this.logger.log(`å¼€å§‹æ‰¹é‡åˆ›å»ºå‘é‡: ${items.length}ä¸ªé¡¹ç›®, ${totalBatches}ä¸ªæ‰¹æ¬¡`);

    for (let i = 0; i < items.length; i += batchSize) {
      const batch = items.slice(i, i + batchSize);
      const batchNumber = Math.floor(i / batchSize) + 1;
      
      try {
        // ç”Ÿæˆæ‰¹é‡å‘é‡
        const contents = batch.map(item => item.content);
        const embeddings = await this.embeddings.embedDocuments(contents);
        
        // å‡†å¤‡æ•°æ®åº“æ’å…¥æ•°æ®
        const vectorData = batch.map((item, index) => ({
          entityType: item.entityType,
          entityId: item.entityId,
          content: item.content,
          embedding: embeddings[index],
          metadata: item.metadata,
        }));

        // ä½¿ç”¨upserté¿å…é‡å¤
        await this.vectorRepository
          .createQueryBuilder()
          .insert()
          .into(VectorEmbedding)
          .values(vectorData)
          .orUpdate(['content', 'embedding', 'metadata', 'updated_at'], ['entity_type', 'entity_id'])
          .execute();

        this.logger.log(`æ‰¹æ¬¡ ${batchNumber}/${totalBatches} å®Œæˆ: ${batch.length}ä¸ªå‘é‡`);
        
        // é¿å…è¿‡å¿«è¯·æ±‚API
        if (i + batchSize < items.length) {
          await this.sleep(1000);
        }
      } catch (error) {
        this.logger.error(`æ‰¹æ¬¡ ${batchNumber} å¤±è´¥`, error);
        throw error;
      }
    }
    
    this.logger.log(`æ‰¹é‡å‘é‡åˆ›å»ºå®Œæˆ: æ€»è®¡${items.length}ä¸ª`);
  }

  /**
   * åˆ é™¤å®ä½“å‘é‡
   */
  async deleteEmbedding(entityType: string, entityId: string): Promise<void> {
    try {
      await this.vectorRepository.delete({ entityType, entityId });
      this.logger.log(`åˆ é™¤å‘é‡åµŒå…¥: ${entityType}/${entityId}`);
    } catch (error) {
      this.logger.error(`åˆ é™¤å‘é‡åµŒå…¥å¤±è´¥: ${entityType}/${entityId}`, error);
      throw error;
    }
  }

  /**
   * è·å–å‘é‡ç»Ÿè®¡ä¿¡æ¯
   */
  async getVectorStats(): Promise<VectorStats[]> {
    try {
      const stats = await this.vectorRepository.query(`
        SELECT * FROM vector_embeddings_stats ORDER BY total_embeddings DESC
      `);
      
      return stats.map(stat => ({
        entityType: stat.entity_type,
        totalEmbeddings: parseInt(stat.total_embeddings),
        newToday: parseInt(stat.new_today),
        updatedToday: parseInt(stat.updated_today),
        firstCreated: stat.first_created,
        lastUpdated: stat.last_updated,
      }));
    } catch (error) {
      this.logger.error('è·å–å‘é‡ç»Ÿè®¡å¤±è´¥', error);
      throw error;
    }
  }

  /**
   * å‘é‡ç´¢å¼•ä¼˜åŒ–
   */
  async optimizeVectorIndex(): Promise<void> {
    try {
      // é‡å»ºå‘é‡ç´¢å¼•
      await this.vectorRepository.query('REINDEX INDEX CONCURRENTLY idx_vector_embeddings_embedding_hnsw');
      
      // æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯
      await this.vectorRepository.query('ANALYZE vector_embeddings');
      
      this.logger.log('å‘é‡ç´¢å¼•ä¼˜åŒ–å®Œæˆ');
    } catch (error) {
      this.logger.error('å‘é‡ç´¢å¼•ä¼˜åŒ–å¤±è´¥', error);
      throw error;
    }
  }

  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

// ç±»å‹å®šä¹‰
interface SimilaritySearchResult {
  entityId: string;
  content: string;
  metadata: any;
  similarity: number;
}

interface VectorStats {
  entityType: string;
  totalEmbeddings: number;
  newToday: number;
  updatedToday: number;
  firstCreated: Date;
  lastUpdated: Date;
}
```

---

## 7. ç›‘æ§å’Œæ—¥å¿—

### 7.1 AIæœåŠ¡ç›‘æ§é…ç½®

```typescript
// monitoring/ai-monitoring.service.ts
import { Injectable, Logger } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { Metrics } from '@opentelemetry/api-metrics';
import { RedisService } from '../services/redis.service';

@Injectable()
export class AIMonitoringService {
  private readonly logger = new Logger(AIMonitoringService.name);
  private readonly metrics: any;

  constructor(
    private configService: ConfigService,
    private redisService: RedisService,
  ) {
    this.initializeMetrics();
  }

  private initializeMetrics() {
    // åˆå§‹åŒ–ç›‘æ§æŒ‡æ ‡
    this.metrics = {
      // APIè°ƒç”¨è®¡æ•°
      apiCalls: this.createCounter('ai_api_calls_total', 'AI APIè°ƒç”¨æ€»æ•°'),
      
      // APIå“åº”æ—¶é—´
      apiLatency: this.createHistogram('ai_api_latency_seconds', 'AI APIå“åº”æ—¶é—´'),
      
      // APIé”™è¯¯è®¡æ•°
      apiErrors: this.createCounter('ai_api_errors_total', 'AI APIé”™è¯¯æ€»æ•°'),
      
      // Tokenä½¿ç”¨é‡
      tokenUsage: this.createCounter('ai_token_usage_total', 'AI Tokenä½¿ç”¨æ€»æ•°'),
      
      // å‘é‡æœç´¢æ€§èƒ½
      vectorSearchLatency: this.createHistogram('vector_search_latency_seconds', 'å‘é‡æœç´¢å“åº”æ—¶é—´'),
      
      // ç¼“å­˜å‘½ä¸­ç‡
      cacheHitRate: this.createGauge('ai_cache_hit_rate', 'AIç¼“å­˜å‘½ä¸­ç‡'),
    };
  }

  /**
   * è®°å½•APIè°ƒç”¨
   */
  recordAPICall(
    service: string,
    model: string,
    success: boolean,
    duration: number,
    tokenUsage?: { prompt: number; completion: number },
  ): void {
    // è®°å½•è°ƒç”¨æ¬¡æ•°
    this.metrics.apiCalls.add(1, {
      service,
      model,
      status: success ? 'success' : 'error',
    });

    // è®°å½•å“åº”æ—¶é—´
    this.metrics.apiLatency.record(duration / 1000, {
      service,
      model,
    });

    // è®°å½•é”™è¯¯
    if (!success) {
      this.metrics.apiErrors.add(1, {
        service,
        model,
      });
    }

    // è®°å½•Tokenä½¿ç”¨
    if (tokenUsage) {
      this.metrics.tokenUsage.add(tokenUsage.prompt, {
        service,
        model,
        type: 'prompt',
      });
      
      this.metrics.tokenUsage.add(tokenUsage.completion, {
        service,
        model,
        type: 'completion',
      });
    }

    // è®°å½•åˆ°Redis (ç”¨äºå®æ—¶ç›‘æ§)
    this.recordToRedis('ai_metrics', {
      timestamp: Date.now(),
      service,
      model,
      success,
      duration,
      tokenUsage,
    });
  }

  /**
   * è®°å½•å‘é‡æœç´¢æ€§èƒ½
   */
  recordVectorSearch(
    entityType: string,
    resultCount: number,
    duration: number,
    similarity: number,
  ): void {
    this.metrics.vectorSearchLatency.record(duration / 1000, {
      entity_type: entityType,
    });

    this.recordToRedis('vector_metrics', {
      timestamp: Date.now(),
      entityType,
      resultCount,
      duration,
      similarity,
    });
  }

  /**
   * è®°å½•ç¼“å­˜æ€§èƒ½
   */
  recordCachePerformance(hit: boolean, service: string): void {
    // æ›´æ–°ç¼“å­˜å‘½ä¸­ç‡
    const key = `cache_stats:${service}`;
    this.updateCacheStats(key, hit);
  }

  /**
   * è·å–AIæœåŠ¡çŠ¶æ€
   */
  async getAIServiceStatus(): Promise<AIServiceStatus> {
    try {
      const now = Date.now();
      const oneHourAgo = now - 60 * 60 * 1000;

      // ä»Redisè·å–æœ€è¿‘ä¸€å°æ—¶çš„æŒ‡æ ‡
      const metrics = await this.getMetricsFromRedis('ai_metrics', oneHourAgo, now);
      
      const totalCalls = metrics.length;
      const successfulCalls = metrics.filter(m => m.success).length;
      const averageLatency = metrics.reduce((sum, m) => sum + m.duration, 0) / totalCalls || 0;
      const totalTokens = metrics.reduce((sum, m) => {
        return sum + (m.tokenUsage?.prompt || 0) + (m.tokenUsage?.completion || 0);
      }, 0);

      return {
        status: successfulCalls / totalCalls > 0.95 ? 'healthy' : 'degraded',
        totalCalls,
        successRate: successfulCalls / totalCalls,
        averageLatency: Math.round(averageLatency),
        totalTokens,
        lastUpdated: new Date(),
      };
    } catch (error) {
      this.logger.error('è·å–AIæœåŠ¡çŠ¶æ€å¤±è´¥', error);
      return {
        status: 'unknown',
        totalCalls: 0,
        successRate: 0,
        averageLatency: 0,
        totalTokens: 0,
        lastUpdated: new Date(),
      };
    }
  }

  /**
   * è·å–æˆæœ¬åˆ†æ
   */
  async getCostAnalysis(timeRange: { start: Date; end: Date }): Promise<CostAnalysis> {
    try {
      const metrics = await this.getMetricsFromRedis(
        'ai_metrics',
        timeRange.start.getTime(),
        timeRange.end.getTime(),
      );

      const costByService = new Map<string, number>();
      let totalCost = 0;

      metrics.forEach(metric => {
        const cost = this.calculateCost(metric.service, metric.model, metric.tokenUsage);
        const serviceCost = costByService.get(metric.service) || 0;
        costByService.set(metric.service, serviceCost + cost);
        totalCost += cost;
      });

      return {
        totalCost,
        costByService: Object.fromEntries(costByService),
        period: timeRange,
        averageCostPerCall: totalCost / metrics.length || 0,
      };
    } catch (error) {
      this.logger.error('è·å–æˆæœ¬åˆ†æå¤±è´¥', error);
      throw error;
    }
  }

  // ç§æœ‰è¾…åŠ©æ–¹æ³•
  private createCounter(name: string, description: string): any {
    // å®é™…å®ç°ä¼šä½¿ç”¨OpenTelemetryçš„Counter
    return {
      add: (value: number, labels?: any) => {
        this.logger.debug(`Counter ${name}: +${value}`, labels);
      },
    };
  }

  private createHistogram(name: string, description: string): any {
    // å®é™…å®ç°ä¼šä½¿ç”¨OpenTelemetryçš„Histogram
    return {
      record: (value: number, labels?: any) => {
        this.logger.debug(`Histogram ${name}: ${value}`, labels);
      },
    };
  }

  private createGauge(name: string, description: string): any {
    // å®é™…å®ç°ä¼šä½¿ç”¨OpenTelemetryçš„Gauge
    return {
      set: (value: number, labels?: any) => {
        this.logger.debug(`Gauge ${name}: ${value}`, labels);
      },
    };
  }

  private async recordToRedis(key: string, data: any): Promise<void> {
    try {
      const timestamp = Date.now();
      await this.redisService.zadd(key, timestamp, JSON.stringify(data));
      
      // åªä¿ç•™æœ€è¿‘7å¤©çš„æ•°æ®
      const sevenDaysAgo = timestamp - 7 * 24 * 60 * 60 * 1000;
      await this.redisService.zremrangebyscore(key, 0, sevenDaysAgo);
    } catch (error) {
      this.logger.error('è®°å½•Redisç›‘æ§æ•°æ®å¤±è´¥', error);
    }
  }

  private async getMetricsFromRedis(
    key: string,
    startTime: number,
    endTime: number,
  ): Promise<any[]> {
    try {
      const rawData = await this.redisService.zrangebyscore(key, startTime, endTime);
      return rawData.map(item => JSON.parse(item));
    } catch (error) {
      this.logger.error('ä»Redisè·å–ç›‘æ§æ•°æ®å¤±è´¥', error);
      return [];
    }
  }

  private async updateCacheStats(key: string, hit: boolean): Promise<void> {
    try {
      const multi = this.redisService.multi();
      multi.hincrby(key, 'total', 1);
      
      if (hit) {
        multi.hincrby(key, 'hits', 1);
      }
      
      await multi.exec();
    } catch (error) {
      this.logger.error('æ›´æ–°ç¼“å­˜ç»Ÿè®¡å¤±è´¥', error);
    }
  }

  private calculateCost(service: string, model: string, tokenUsage?: any): number {
    // æ ¹æ®ä¸åŒæœåŠ¡å’Œæ¨¡å‹è®¡ç®—æˆæœ¬
    const costPerToken = {
      'deepseek-chat': {
        prompt: 0.0000014, // $0.0014 per 1K tokens
        completion: 0.0000028, // $0.0028 per 1K tokens
      },
      'text-embedding-ada-002': {
        prompt: 0.0000001, // $0.0001 per 1K tokens
        completion: 0,
      },
    };

    if (!tokenUsage || !costPerToken[model]) {
      return 0;
    }

    const promptCost = (tokenUsage.prompt || 0) * costPerToken[model].prompt;
    const completionCost = (tokenUsage.completion || 0) * costPerToken[model].completion;
    
    return promptCost + completionCost;
  }
}

// ç±»å‹å®šä¹‰
interface AIServiceStatus {
  status: 'healthy' | 'degraded' | 'down' | 'unknown';
  totalCalls: number;
  successRate: number;
  averageLatency: number;
  totalTokens: number;
  lastUpdated: Date;
}

interface CostAnalysis {
  totalCost: number;
  costByService: Record<string, number>;
  period: { start: Date; end: Date };
  averageCostPerCall: number;
}
```

---

## 8. ç¯å¢ƒå˜é‡é…ç½®

### 8.1 ç¯å¢ƒå˜é‡æ¸…å•

```bash
# .env.example - AIæœåŠ¡é…ç½®

# DeepSeek APIé…ç½®
DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_API_BASE_URL=https://api.deepseek.com
DEEPSEEK_CHAT_MODEL=deepseek-chat
DEEPSEEK_CODER_MODEL=deepseek-coder
DEEPSEEK_MAX_TOKENS=4096
DEEPSEEK_TEMPERATURE=0.7
DEEPSEEK_TIMEOUT=30000
DEEPSEEK_MAX_RETRIES=3
DEEPSEEK_BACKOFF_MULTIPLIER=2
DEEPSEEK_MAX_BACKOFF=60000
DEEPSEEK_RPM_LIMIT=60
DEEPSEEK_RPD_LIMIT=10000
DEEPSEEK_CACHE_ENABLED=true
DEEPSEEK_CACHE_TTL=3600
DEEPSEEK_CACHE_PREFIX=ds:

# OpenAI APIé…ç½® (ç”¨äºå‘é‡åŒ–)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_EMBEDDING_MODEL=text-embedding-ada-002
OPENAI_MAX_RETRIES=3
OPENAI_TIMEOUT=30000

# LangChainé…ç½®
LANGCHAIN_CACHE_ENABLED=true
LANGCHAIN_CACHE_TTL=3600
LANGCHAIN_CACHE_MAX_SIZE=1000
LANGCHAIN_CACHE_PREFIX=lc:

# å‘é‡æ•°æ®åº“é…ç½®
VECTOR_TABLE_NAME=vector_embeddings
VECTOR_DIMENSIONS=1536
VECTOR_INDEX_TYPE=hnsw
VECTOR_INDEX_LISTS=100
VECTOR_INDEX_M=16
VECTOR_INDEX_EF_CONSTRUCTION=64

# AIç›‘æ§é…ç½®
AI_MONITORING_ENABLED=true
AI_METRICS_RETENTION_DAYS=7
AI_COST_TRACKING_ENABLED=true

# é™æµé…ç½®
AI_RATE_LIMIT_ENABLED=true
AI_RATE_LIMIT_WINDOW=60000
AI_RATE_LIMIT_MAX_REQUESTS=100

# é”™è¯¯å¤„ç†é…ç½®
AI_CIRCUIT_BREAKER_ENABLED=true
AI_CIRCUIT_BREAKER_THRESHOLD=10
AI_CIRCUIT_BREAKER_TIMEOUT=60000

# æ—¥å¿—é…ç½®
AI_LOG_LEVEL=info
AI_LOG_REQUESTS=false
AI_LOG_RESPONSES=false
AI_LOG_ERRORS=true
```

### 8.2 é…ç½®éªŒè¯

```typescript
// config/ai-config.validation.ts
import { IsString, IsNumber, IsBoolean, IsOptional, Min, Max } from 'class-validator';
import { Transform } from 'class-transformer';

export class AIConfigValidation {
  @IsString()
  DEEPSEEK_API_KEY: string;

  @IsString()
  @IsOptional()
  DEEPSEEK_API_BASE_URL: string = 'https://api.deepseek.com';

  @IsString()
  @IsOptional()
  DEEPSEEK_CHAT_MODEL: string = 'deepseek-chat';

  @Transform(({ value }) => parseInt(value))
  @IsNumber()
  @Min(1)
  @Max(8192)
  @IsOptional()
  DEEPSEEK_MAX_TOKENS: number = 4096;

  @Transform(({ value }) => parseFloat(value))
  @IsNumber()
  @Min(0)
  @Max(2)
  @IsOptional()
  DEEPSEEK_TEMPERATURE: number = 0.7;

  @Transform(({ value }) => parseInt(value))
  @IsNumber()
  @Min(1000)
  @IsOptional()
  DEEPSEEK_TIMEOUT: number = 30000;

  @IsString()
  OPENAI_API_KEY: string;

  @Transform(({ value }) => value === 'true')
  @IsBoolean()
  @IsOptional()
  LANGCHAIN_CACHE_ENABLED: boolean = true;

  @Transform(({ value }) => parseInt(value))
  @IsNumber()
  @Min(1536)
  @Max(1536)
  @IsOptional()
  VECTOR_DIMENSIONS: number = 1536;
}
```

---

## æ–‡æ¡£è¯´æ˜

æœ¬AIæœåŠ¡é›†æˆé…ç½®æ–‡æ¡£æä¾›äº†å®Œæ•´çš„AIæŠ€æœ¯æ ˆé›†æˆæ–¹æ¡ˆï¼ŒåŒ…æ‹¬ï¼š

1. **LangChainé›†æˆ** - å®Œæ•´çš„LangChainæ¡†æ¶é…ç½®å’ŒæœåŠ¡å®ç°
2. **DeepSeek API** - DeepSeekå¤§æ¨¡å‹APIçš„é›†æˆé…ç½®å’Œä½¿ç”¨æ–¹æ³•
3. **å‘é‡æ•°æ®åº“** - pgvectoræ‰©å±•é…ç½®å’Œå‘é‡æ“ä½œæœåŠ¡
4. **ç›‘æ§ä½“ç³»** - AIæœåŠ¡çš„å®Œæ•´ç›‘æ§å’Œæˆæœ¬åˆ†æ
5. **é…ç½®ç®¡ç†** - ç¯å¢ƒå˜é‡é…ç½®å’ŒéªŒè¯æœºåˆ¶

æ‰€æœ‰é…ç½®éƒ½åŸºäºç”Ÿäº§ç¯å¢ƒçš„æœ€ä½³å®è·µï¼Œç¡®ä¿AIæœåŠ¡çš„ç¨³å®šæ€§ã€æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚å¼€å‘å›¢é˜Ÿåº”ä¸¥æ ¼æŒ‰ç…§æ­¤é…ç½®å®æ–½AIæœåŠ¡é›†æˆã€‚